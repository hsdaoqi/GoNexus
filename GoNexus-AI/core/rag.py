import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from core.config import settings


class RAGEngine:
    def __init__(self):
        print("ğŸš€ [Init] æ­£åœ¨å¯åŠ¨ RAG å¼•æ“...", flush=True)

        # 1. è¿æ¥ AI
        self.ai_client = OpenAI(
            api_key=settings.API_KEY,
            base_url=settings.BASE_URL
        )

        # 2. è¿æ¥æ•°æ®åº“
        print("--- æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“ (é¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™)... ---", flush=True)
        self.chroma_client = chromadb.PersistentClient(path=settings.DB_PATH)

        # 3. åŠ è½½æ¨¡å‹
        emb_fn = embedding_functions.DefaultEmbeddingFunction()

        self.collection = self.chroma_client.get_or_create_collection(
            name="chat_memory",
            embedding_function=emb_fn
        )
        print("âœ… [Init] RAG å¼•æ“å¯åŠ¨æˆåŠŸï¼", flush=True)

    # å­˜å…¥è®°å¿† (å¸¦æ˜µç§°)
    def add_memory(self, text: str, user_id: str, msg_id: str, nickname: str, session_id: str):
        print(f"ğŸ“¥ [è®°å¿†] ä¼šè¯:{session_id} {nickname}: {text}")
        self.collection.add(
            documents=[text],
            metadatas=[{"user_id": user_id, "user": nickname, "session_id": session_id}],
            ids=[msg_id]
        )

    # æ£€ç´¢è®°å¿†
    def search_memory(self, query: str, session_id: str, limit: int = 3):
        print(f"ğŸ” [æ£€ç´¢] ä¼šè¯:{session_id} é—®é¢˜:{query}")
        results = self.collection.query(
            query_texts=[query],
            n_results=limit,
            # æ ¸å¿ƒå®‰å…¨é”ï¼šåªæœè¿™ä¸ª session_id çš„æ•°æ®
            where={"session_id": session_id}
        )

        if not results['documents'] or not results['documents'][0]:
            return []

        docs = results['documents'][0]
        metas = results['metadatas'][0]
        combined = []

        for i in range(len(docs)):
            # ä¼˜å…ˆå–æ˜µç§°
            name = metas[i].get('user', metas[i].get('user_id', 'æœªçŸ¥'))
            text = docs[i]
            combined.append(f"{name}: {text}")

        return combined

    # RAG é—®ç­”
    def rag_qa(self, question: str, session_id: str, limit: int):
        # 1. æ£€ç´¢ (æŠŠ limit æ”¹å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚ 10 æ¡)
        # æ—¢ç„¶æ˜¯èŠå¤©è®°å½•ï¼Œä¸Šä¸‹æ–‡å¤šä¸€ç‚¹æ²¡åå¤„
        related_docs = self.search_memory(question, session_id, limit)

        # ğŸ”¥ğŸ”¥ğŸ”¥ã€è°ƒè¯•ä»£ç ã€‘æ‰“å°å‡º AI åˆ°åº•çœ‹åˆ°äº†ä»€ä¹ˆ
        print(f"ğŸ§ [Debug] ç”¨æˆ·é—®: {question}")
        print(f"ğŸ§ [Debug] æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡: {related_docs}")
        # ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

        if not related_docs:
            return "æŠ±æ­‰ï¼Œæˆ‘çš„è®°å¿†åº“é‡Œæ²¡æœ‰å…³äºè¿™ä»¶äº‹çš„è®°å½•ã€‚"

        context = "\n".join(related_docs)

        # 2. ç”Ÿæˆ (Prompt å¾®è°ƒï¼šè®© AI æ›´èªæ˜ä¸€ç‚¹)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªèŠå¤©è®°å½•åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ã€å‚è€ƒè®°å½•ã€‘å›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚

        æ³¨æ„ï¼š
        1. è®°å½•æ ¼å¼ä¸º "å§“å: å†…å®¹"ã€‚
        2. "æˆ‘" æŒ‡çš„æ˜¯å‘è¨€è€…ã€‚ä¾‹å¦‚ "Alice: æˆ‘é¥¿äº†" æ„å‘³ç€ Alice é¥¿äº†ã€‚
        3. è¯·æ ¹æ®ä¸Šä¸‹æ–‡é€»è¾‘æ¨ç†ã€‚

        ã€å‚è€ƒè®°å½•ã€‘ï¼š
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{question}
        """

        # 3. æé—® AI
        print(f"ğŸ¤– [æ€è€ƒ] æ­£åœ¨è¯·æ±‚ AI...")
        response = self.ai_client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    # æ€»ç»“
    def chat_summary(self, chats: list):
        context = "\n".join(chats)
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹èŠå¤©å†…å®¹ï¼š\n{context}"
        response = self.ai_client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
