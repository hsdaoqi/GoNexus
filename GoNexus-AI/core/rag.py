import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from core.config import settings


class RAGEngine:
    def __init__(self):
        print("ğŸš€ [Init] æ­£åœ¨å¯åŠ¨ RAG å¼•æ“...", flush=True)

        # 1. è¿æ¥ AI
        self.ai_client = OpenAI(
            api_key=settings.API_KEY,
            base_url=settings.BASE_URL
        )

        # 2. è¿æ¥æ•°æ®åº“
        print("--- æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“ (é¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™)... ---", flush=True)
        self.chroma_client = chromadb.PersistentClient(path=settings.DB_PATH)

        # 3. åŠ è½½æ¨¡å‹
        emb_fn = embedding_functions.DefaultEmbeddingFunction()

        self.collection = self.chroma_client.get_or_create_collection(
            name="chat_memory",
            embedding_function=emb_fn
        )
        print("âœ… [Init] RAG å¼•æ“å¯åŠ¨æˆåŠŸï¼", flush=True)

    # å­˜å…¥è®°å¿† (å¸¦æ˜µç§°)
    def add_memory(self, text: str, user_id: str, msg_id: str, nickname: str, session_id: str):
        print(f"ğŸ“¥ [è®°å¿†] ä¼šè¯:{session_id} {nickname}: {text}")
        self.collection.add(
            documents=[text],
            metadatas=[{"user_id": user_id, "user": nickname, "session_id": session_id}],
            ids=[msg_id]
        )

    # æ’¤å›è®°å¿†
    def revoke_memory(self, msg_id: str):
        print(f"ğŸ—‘ï¸ [æ’¤å›] ç§»é™¤è®°å¿† ID: {msg_id}")
        self.collection.delete(ids=[msg_id])

    # æ£€ç´¢è®°å¿†
    def search_memory(self, query: str, session_id: str, limit: int = 3):
        print(f"ğŸ” [æ£€ç´¢] ä¼šè¯:{session_id} é—®é¢˜:{query}")
        results = self.collection.query(
            query_texts=[query],
            n_results=limit,
            # æ ¸å¿ƒå®‰å…¨é”ï¼šåªæœè¿™ä¸ª session_id çš„æ•°æ®
            where={"session_id": session_id}
        )

        if not results['documents'] or not results['documents'][0]:
            return []

        docs = results['documents'][0]
        metas = results['metadatas'][0]
        combined = []

        for i in range(len(docs)):
            # ä¼˜å…ˆå–æ˜µç§°
            name = metas[i].get('user', metas[i].get('user_id', 'æœªçŸ¥'))
            text = docs[i]
            combined.append(f"{name}: {text}")

        return combined

    # RAG é—®ç­”
    def rag_qa(self, question: str, session_id: str, limit: int):
        # 1. æ£€ç´¢ (æŠŠ limit æ”¹å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚ 10 æ¡)
        # æ—¢ç„¶æ˜¯èŠå¤©è®°å½•ï¼Œä¸Šä¸‹æ–‡å¤šä¸€ç‚¹æ²¡åå¤„
        related_docs = self.search_memory(question, session_id, limit)

        # ğŸ”¥ğŸ”¥ğŸ”¥ã€è°ƒè¯•ä»£ç ã€‘æ‰“å°å‡º AI åˆ°åº•çœ‹åˆ°äº†ä»€ä¹ˆ
        print(f"ğŸ§ [Debug] ç”¨æˆ·é—®: {question}")
        print(f"ğŸ§ [Debug] æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡: {related_docs}")
        # ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

        if not related_docs:
            return "æŠ±æ­‰ï¼Œæˆ‘çš„è®°å¿†åº“é‡Œæ²¡æœ‰å…³äºè¿™ä»¶äº‹çš„è®°å½•ã€‚"

        context = "\n".join(related_docs)

        # 2. ç”Ÿæˆ (Prompt å¾®è°ƒï¼šè®© AI æ›´èªæ˜ä¸€ç‚¹)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªèŠå¤©è®°å½•åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ã€å‚è€ƒè®°å½•ã€‘å›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚

        æ³¨æ„ï¼š
        1. è®°å½•æ ¼å¼ä¸º "å§“å: å†…å®¹"ã€‚
        2. "æˆ‘" æŒ‡çš„æ˜¯å‘è¨€è€…ã€‚ä¾‹å¦‚ "Alice: æˆ‘é¥¿äº†" æ„å‘³ç€ Alice é¥¿äº†ã€‚
        3. è¯·æ ¹æ®ä¸Šä¸‹æ–‡é€»è¾‘æ¨ç†ã€‚

        ã€å‚è€ƒè®°å½•ã€‘ï¼š
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{question}
        """

        # 3. æé—® AI
        print(f"ğŸ¤– [æ€è€ƒ] æ­£åœ¨è¯·æ±‚ AI...")
        response = self.ai_client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    # æ€»ç»“
    def chat_summary(self, chats: list):
        context = "\n".join(chats)
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹èŠå¤©å†…å®¹ï¼š\n{context}"
        response = self.ai_client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    # å›å¤å»ºè®®
    def reply_suggestion(self, recent_messages: list, my_name: str):
        context = "\n".join(recent_messages)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯è®°å½•ï¼Œä¸ºã€{my_name}ã€‘ç”Ÿæˆ 3 ä¸ªç®€çŸ­ã€è‡ªç„¶ã€å¾—ä½“çš„å›å¤å»ºè®®ã€‚
        
        è¦æ±‚ï¼š
        1. å›å¤è¦ç¬¦åˆè¯­å¢ƒã€‚
        2. æ¯ä¸ªå»ºè®®ä¸è¶…è¿‡ 15 ä¸ªå­—ã€‚
        3. ç›´æ¥è¿”å› 3 ä¸ªå»ºè®®ï¼Œç”¨ "|" åˆ†éš”ï¼Œä¸è¦åŒ…å«åºå·æˆ–å…¶ä»–åºŸè¯ã€‚
        
        ã€å¯¹è¯è®°å½•ã€‘ï¼š
        {context}
        """
        
        print(f"ğŸ’¡ [å»ºè®®] ä¸º {my_name} ç”Ÿæˆå›å¤å»ºè®®...")
        response = self.ai_client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.choices[0].message.content.strip()
        # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„é¢å¤–å­—ç¬¦
        suggestions = [s.strip() for s in content.split('|') if s.strip()]
        # å¦‚æœ AI æ²¡æŒ‰æ ¼å¼è¿”å›ï¼Œå°è¯•ç”¨æ¢è¡Œç¬¦åˆ†å‰²
        if len(suggestions) < 2 and '\n' in content:
             suggestions = [s.strip() for s in content.split('\n') if s.strip()]
             
        return suggestions[:3] # ç¡®ä¿åªè¿”å›å‰3ä¸ª
